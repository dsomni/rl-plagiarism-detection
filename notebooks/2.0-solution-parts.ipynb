{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "- https://coderzcolumn.com/tutorials/artificial-intelligence/how-to-use-glove-embeddings-with-pytorch\n",
    "\n",
    "- https://ojs.aaai.org/index.php/AAAI/article/view/12047\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEVICE = torch.device(\"cuda\") or torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../data\\glove.840B.300d.zip: 2.18GB [08:04, 4.50MB/s]                                \n",
      "100%|█████████▉| 2196016/2196017 [06:17<00:00, 5810.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "print(tokenizer(\"Hello, world?\"))\n",
    "\n",
    "global_vectors = GloVe(cache=\"../data\")\n",
    "\n",
    "global_vectors.get_vecs_by_tokens(tokenizer(\"Hello, world?\"), lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"hello, world!\", \"hi world\", 1.0),\n",
    "    (\"hello, world!\", \"cat eats beetle\", 0.0),\n",
    "    (\"sky is blue\", \"horizon is not red\", 0.5),\n",
    "    (\"sky is red\", \"horizon is not blue\", 0.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlagiarismDataset(Dataset):\n",
    "    def __init__(self, data: list[tuple[str, str, int]]):\n",
    "        self.data = np.array(data)\n",
    "\n",
    "        self.targets = self.data[:, 0]\n",
    "        self.candidates = self.data[:, 1]\n",
    "        self.scores = self.data[:, 2].astype(np.float16)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scores)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.targets[idx], self.candidates[idx], self.scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "('hello, world!', 'hi world', 1.0)\n"
     ]
    }
   ],
   "source": [
    "dataset = PlagiarismDataset(data)\n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2523,  0.1018, -0.6748,  ...,  0.1787, -0.5192,  0.3359],\n",
       "         [-0.0828,  0.6720, -0.1499,  ..., -0.1918, -0.3785, -0.0659],\n",
       "         [-0.0067,  0.2224,  0.2771,  ...,  0.0594,  0.0014,  0.0987],\n",
       "         [-0.2655,  0.3353,  0.2186,  ..., -0.1786, -0.0629,  0.1623]]),\n",
       " tensor([[ 2.8796e-02,  4.1306e-01, -4.6690e-01, -7.8175e-02,  3.7058e-01,\n",
       "           1.2867e-01,  4.7714e-01, -9.2372e-01, -6.7789e-02,  6.2381e-01,\n",
       "          -2.9670e-01, -4.4328e-01, -8.4224e-02, -3.1270e-01, -1.8197e-01,\n",
       "           3.2360e-01, -7.7793e-02,  1.3314e+00, -1.5676e-01,  1.2857e-01,\n",
       "           4.3474e-02,  7.9883e-02,  1.1311e-02,  1.4428e-01,  1.7653e-01,\n",
       "          -2.2321e-01, -4.2480e-02,  2.1707e-03, -4.7640e-02,  3.8532e-01,\n",
       "          -5.9911e-02,  1.8338e-01, -1.9145e-01, -1.3184e-01, -2.2440e-01,\n",
       "          -3.4313e-01, -1.9527e-01,  2.0129e-01, -2.8915e-01, -2.0750e-01,\n",
       "           1.9230e-01, -4.3318e-01, -3.5914e-02, -1.7492e-01,  5.1793e-03,\n",
       "           4.1998e-01,  1.0637e-01,  1.6559e-01,  2.8926e-01,  2.1868e-01,\n",
       "          -7.7643e-02,  6.1037e-01, -1.7432e-02, -2.9676e-03, -3.0160e-01,\n",
       "          -1.1983e-02, -9.4832e-02,  9.5424e-02, -3.7713e-01, -1.1239e-01,\n",
       "          -7.8399e-01, -1.7278e-01,  4.9498e-02, -2.0969e-01,  3.1968e-01,\n",
       "          -3.0732e-01,  1.0192e-01,  2.0580e-01,  3.2505e-01, -2.5291e-01,\n",
       "          -9.3692e-02,  5.2662e-03,  4.5696e-01, -1.1763e-01,  2.6193e-01,\n",
       "           3.2966e-02, -4.7883e-03,  4.7738e-01, -3.3887e-02,  3.6247e-01,\n",
       "          -1.9945e-01,  4.4342e-01, -3.7178e-01,  3.2319e-01, -1.1709e-01,\n",
       "          -1.5551e-01,  1.4257e+00, -4.7203e-01,  2.4915e-01,  1.2907e-01,\n",
       "           1.3357e-01, -1.5880e-01, -3.0594e-01, -9.4597e-02,  1.3255e-01,\n",
       "          -8.9818e-02,  5.0826e-01, -2.0685e-01, -6.9602e-01,  4.8778e-01,\n",
       "          -1.4408e-01,  5.1481e-02, -1.6557e-02,  3.3421e-01,  6.7242e-02,\n",
       "          -1.1685e-01, -4.6423e-02, -3.9958e-01, -3.1008e-01, -2.4609e-01,\n",
       "           5.8174e-02, -5.2140e-01, -6.0439e-02,  3.0534e-03,  4.1036e-01,\n",
       "           4.4092e-01,  2.8334e-01, -5.6422e-01,  5.5707e-02,  1.7791e-04,\n",
       "           3.4433e-01, -3.0717e-01,  2.5623e-01, -2.6241e-01,  4.0216e-01,\n",
       "           3.3964e-01,  5.5718e-01,  6.9994e-02, -1.6490e-01,  3.2947e-01,\n",
       "          -6.9621e-02, -3.7227e-01, -1.0987e-01, -3.7106e-01,  4.0310e-01,\n",
       "          -4.1511e-01, -9.0917e-02,  1.7001e-01, -3.4748e-01, -1.6285e-01,\n",
       "          -2.3767e+00, -3.5290e-01, -8.5539e-02, -5.0965e-01, -1.5912e-01,\n",
       "           2.4123e-01, -2.0030e-01,  2.9155e-01, -3.3438e-01, -2.1440e-01,\n",
       "           1.0519e-01, -6.0930e-02, -3.5564e-01, -3.5314e-01,  1.1538e-01,\n",
       "           1.3500e-01,  3.1325e-01, -1.0790e-01,  2.4903e-01, -4.0942e-01,\n",
       "           1.9815e-01,  1.5635e-01,  4.5990e-01, -1.7499e-04, -8.7480e-02,\n",
       "           3.3567e-02,  1.2889e-01, -1.3793e-02, -1.3751e-01,  3.8376e-01,\n",
       "          -4.8534e-01,  1.0498e-01, -3.0883e-01, -3.9634e-01, -7.5734e-02,\n",
       "          -3.9470e-01, -3.3696e-01,  2.5969e-02, -2.9933e-02,  2.1998e-01,\n",
       "          -3.6887e-01, -6.3065e-02, -4.5264e-01, -7.6559e-02, -9.0896e-02,\n",
       "          -2.7469e-01,  2.3256e-01, -6.9002e-02,  8.1259e-02, -2.9682e-01,\n",
       "           5.0958e-01, -2.4812e-01, -4.1866e-01, -4.9677e-01,  5.3641e-02,\n",
       "          -1.6098e-01, -1.3070e-01, -2.1058e-01,  7.0593e-01,  4.1502e-01,\n",
       "          -2.9617e-01, -1.2387e-01,  1.9504e-02, -2.1288e-01,  6.1103e-02,\n",
       "          -4.0131e-01,  5.3975e-01, -3.7639e-01, -1.8536e-01, -3.6357e-01,\n",
       "          -4.5547e-01, -9.0210e-02, -2.0425e-01, -2.4413e-01, -8.1124e-02,\n",
       "           2.4698e-02,  7.5438e-02,  6.2125e-03,  1.6757e-01, -2.1207e-01,\n",
       "          -6.1182e-02,  4.4722e-01,  4.1641e-01,  8.2606e-01, -6.2413e-03,\n",
       "           5.5281e-01, -1.5134e-01, -7.9939e-02,  8.4223e-02, -3.3734e-01,\n",
       "          -2.3321e-02, -2.9588e-01, -9.3586e-01,  3.2397e-01, -2.4314e-01,\n",
       "          -2.0533e-02, -5.2084e-01,  5.2986e-02, -1.1679e-01,  4.7422e-01,\n",
       "          -1.8861e-01,  2.8550e-01,  5.2586e-01,  3.2893e-01,  3.1098e-01,\n",
       "          -1.5665e-01,  5.8859e-01,  1.2991e-01,  4.8790e-01,  7.1808e-02,\n",
       "           1.3260e-01,  1.6146e-01, -4.9939e-01, -1.5210e-01,  7.6596e-02,\n",
       "           3.7449e-01, -1.8812e-01,  1.6209e-01, -3.0729e-01, -3.8459e-01,\n",
       "          -8.6934e-02, -3.4415e-01,  2.1309e-01,  9.9894e-02, -9.2105e-01,\n",
       "          -2.6550e-01,  5.2581e-03,  8.0952e-01, -2.6002e-01, -1.9374e-01,\n",
       "          -4.7203e-01,  4.0053e-01, -1.3437e-01,  2.0369e-01,  1.2778e-01,\n",
       "          -5.7577e-02,  2.4322e-03, -4.2885e-02, -8.2562e-02,  2.7829e-01,\n",
       "           4.3434e-02, -9.3094e-02, -3.0028e-01,  1.9869e-01, -2.7712e-02,\n",
       "          -2.8615e-01,  5.4265e-02,  1.7516e-01,  9.4575e-02,  4.7020e-01,\n",
       "           3.6270e-01, -2.0331e-01, -3.2928e-01, -4.8915e-02,  6.3414e-01,\n",
       "          -1.1668e-01,  2.0476e-01, -5.3029e-02, -3.3494e-01,  3.6282e-01],\n",
       "         [-6.6796e-03,  2.2238e-01,  2.7709e-01, -1.6760e-01,  3.9934e-01,\n",
       "          -2.6935e-01,  1.1758e-01,  8.2171e-01, -2.9600e-01,  2.7338e+00,\n",
       "          -5.1854e-01, -3.9980e-01,  4.5268e-02,  7.0994e-02,  3.8210e-01,\n",
       "          -1.9731e-01, -4.1031e-01,  9.1488e-01, -4.5642e-02,  1.9991e-01,\n",
       "           3.5154e-02,  4.0459e-02,  5.2444e-02,  1.6124e-01,  1.8280e-01,\n",
       "          -1.5186e-01,  3.1278e-01, -1.4228e-02, -7.5154e-02,  6.3618e-01,\n",
       "          -3.0658e-03,  1.1577e-01, -1.5101e-02, -9.5193e-02,  4.2313e-01,\n",
       "          -3.5243e-01,  1.5452e-01, -3.9491e-01,  6.1847e-02,  4.5658e-01,\n",
       "           1.8395e-01, -3.4329e-02,  8.0296e-02,  2.1393e-01, -1.2170e-01,\n",
       "           1.2957e-02, -2.3259e-01, -8.4731e-02,  4.4998e-02, -1.4019e-01,\n",
       "          -7.0056e-02, -1.2399e-01, -4.5609e-01, -1.0087e-01,  3.9152e-01,\n",
       "          -1.9437e-01,  3.1484e-01, -2.4423e-01,  7.8843e-01, -2.7220e-01,\n",
       "           3.5652e-02,  7.1816e-02, -9.9133e-02,  1.0260e-02, -1.4698e-01,\n",
       "           1.9898e-02,  1.5508e-01, -9.4629e-02, -1.1017e-01,  3.4240e-02,\n",
       "          -1.9389e-01, -4.3287e-01,  2.9240e-01,  2.0294e-01, -4.4540e-01,\n",
       "           3.3994e-01,  8.8975e-03, -1.5746e-02,  4.4674e-01,  4.8578e-01,\n",
       "           4.7199e-02,  2.3850e-01, -1.7062e-01,  1.3774e-01, -7.3939e-02,\n",
       "          -8.5463e-01, -9.5668e-01, -2.5329e-01, -1.1777e-01, -1.1812e-01,\n",
       "          -5.2252e-01,  2.0321e-01, -6.3754e-01,  1.3490e-01, -1.8466e-01,\n",
       "          -1.8157e-01,  2.6867e-02, -2.1901e-02,  2.6663e-01,  2.7428e-01,\n",
       "           3.3822e-01,  8.2030e-02, -2.8889e-01, -7.3748e-02, -8.9901e-03,\n",
       "          -1.0196e+00, -1.5378e-02,  9.8236e-02, -1.8926e-01,  1.0841e-01,\n",
       "           2.5414e-01, -2.0826e-01, -7.5681e-02, -2.2413e-01, -1.8159e-01,\n",
       "           4.6597e-01,  2.0012e-01,  2.8821e-01, -6.3588e-02, -2.5630e-01,\n",
       "          -2.1808e-01, -1.5706e-01,  2.6271e-01, -4.1973e-01,  1.0095e-01,\n",
       "           1.6371e-01,  2.2834e-01, -1.2602e-01,  3.7693e-01, -2.3717e-01,\n",
       "           4.2714e-01, -1.2639e-01,  2.1220e-01, -3.8119e-01, -9.5456e-02,\n",
       "          -2.6961e-01, -1.3042e-01, -2.1551e-01, -1.8953e-01,  3.0259e-01,\n",
       "          -1.9741e+00, -5.2030e-01,  2.9876e-01,  5.7871e-02,  2.1359e-01,\n",
       "          -3.4952e-01,  5.3584e-01,  3.6520e-01, -3.7080e-01, -6.3623e-02,\n",
       "           2.5807e-01, -2.6754e-02,  3.1513e-01,  2.6846e-01,  6.5093e-02,\n",
       "           2.5803e-04, -1.3588e-01,  2.4980e-01, -1.3909e-01,  1.1234e-01,\n",
       "           3.3316e-01,  3.4609e-01, -3.0484e-02,  1.9027e-01,  2.0124e-01,\n",
       "          -2.0287e-01,  3.0843e-01, -4.6492e-01,  2.7708e-01,  2.9172e-01,\n",
       "          -9.7622e-03, -3.0099e-01,  7.6570e-02, -9.7341e-02, -1.2393e-01,\n",
       "           1.4921e-01, -1.6147e-01, -1.7466e-01, -1.5586e-01,  2.1108e-01,\n",
       "           4.9188e-01,  2.9547e-01,  1.4422e-01, -3.3139e-01,  2.6869e-03,\n",
       "          -3.2858e-01,  5.1358e-03, -1.0870e-01,  1.9086e-02, -7.8923e-02,\n",
       "          -4.0630e-01, -2.0526e-01,  4.2546e-01,  2.9473e-01, -2.9929e-01,\n",
       "           6.7783e-02,  1.7279e-01,  1.6008e-01,  1.9886e-01,  4.5809e-02,\n",
       "           7.2766e-02, -8.6075e-02,  3.4542e-01,  2.8371e-01,  3.5538e-01,\n",
       "          -2.5499e-01,  3.1744e-01,  5.9334e-03, -3.7372e-01, -2.0612e-01,\n",
       "          -5.2648e-01, -7.4487e-01,  1.1757e-02, -3.0472e-01,  2.6317e-02,\n",
       "          -1.3342e-01,  3.0670e-01, -2.5618e-01, -1.8582e-01,  1.9382e-02,\n",
       "          -2.9999e-01,  2.0616e-01,  1.8106e-01, -1.3167e-01, -3.1513e-01,\n",
       "           2.1602e-01,  8.3713e-03,  3.5770e-01, -4.7267e-01,  3.9963e-01,\n",
       "           5.3379e-02,  1.7932e-01,  2.6909e-02, -4.4594e-02, -2.9042e-01,\n",
       "           9.6057e-02, -6.7086e-01, -1.8504e-01, -3.7122e-01, -3.7638e-01,\n",
       "           2.4558e-01,  2.5896e-01,  1.7423e-01,  2.3388e-01, -4.0986e-02,\n",
       "          -2.2366e-01, -2.4149e-01,  3.3690e-01, -5.1075e-01, -3.7846e-02,\n",
       "           3.4640e-01, -2.8095e-01,  7.0234e-02,  1.5570e-02, -1.6339e-01,\n",
       "          -4.6059e-03,  2.3066e-01,  3.4541e-01, -1.0091e-01,  3.8429e-01,\n",
       "           2.1258e-01,  2.6249e-01, -2.2807e-01,  1.6382e-01,  1.3648e-02,\n",
       "          -6.0375e-01, -1.5751e-01, -1.1807e-01,  6.0896e-01,  4.4867e-01,\n",
       "          -6.8183e-01, -3.7484e-01, -2.6337e-01,  1.9173e-02,  1.4897e-01,\n",
       "          -9.4622e-02, -8.9517e-02,  2.5521e-01,  1.9849e-01, -3.1701e-01,\n",
       "           4.1117e-02, -3.1162e-01,  6.1004e-02, -5.2715e-03, -1.0638e-01,\n",
       "          -2.7631e-01,  2.1142e-02, -1.1392e-01,  2.4777e-02, -6.5339e-02,\n",
       "          -6.2834e-01, -1.4461e-02, -1.4295e-01, -2.0125e-01, -6.6727e-02,\n",
       "           3.7398e-01,  3.1205e-02,  5.9372e-02,  1.4085e-03,  9.8727e-02]]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_pipeline(x):\n",
    "    return global_vectors.get_vecs_by_tokens(tokenizer(x), lower_case_backup=True)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    target_list, candidate_list, score_list = [], [], []\n",
    "    for _target, _candidate, _score in batch:\n",
    "        target_list.append(text_pipeline(_target))\n",
    "        candidate_list.append(text_pipeline(_candidate))\n",
    "        score_list.append(_score)\n",
    "\n",
    "    target_list = torch.cat(target_list).float()\n",
    "    candidate_list = torch.cat(candidate_list).float()\n",
    "    score_list = torch.tensor(score_list).float()\n",
    "    return target_list.to(DEVICE), candidate_list.to(DEVICE), score_list.to(DEVICE)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = global_vectors.dim\n",
    "LSTM_LAYERS = 2\n",
    "HIDDEN_SIZE = EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(EMBED_DIM, HIDDEN_SIZE, num_layers=LSTM_LAYERS, bidirectional=False).to(\n",
    "    DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_loop(sent, lstm):\n",
    "    h = torch.zeros((LSTM_LAYERS, HIDDEN_SIZE)).to(DEVICE)\n",
    "    c = torch.zeros((LSTM_LAYERS, HIDDEN_SIZE)).to(DEVICE)\n",
    "    for t in sent:\n",
    "        torch.cat([h.flatten(), c.flatten(), t])\n",
    "        # print(state.shape, state[:10])\n",
    "        _, (h, c) = lstm(t.view(1, -1))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One by one\n",
    "\n",
    "torch.manual_seed(42)\n",
    "inputs = [torch.randn(300).to(DEVICE) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "h_c = (\n",
    "    torch.zeros(LSTM_LAYERS, HIDDEN_SIZE).to(DEVICE),\n",
    "    torch.zeros(LSTM_LAYERS, HIDDEN_SIZE).to(DEVICE),\n",
    ")\n",
    "for i in inputs:\n",
    "    out, h_c = lstm(i.view(1, -1), h_c)\n",
    "    print(out[0, :3], h_c[0][0, :3], h_c[1][0, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All at once\n",
    "\n",
    "inputs2 = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "h_c = (\n",
    "    torch.zeros(LSTM_LAYERS, 1, HIDDEN_SIZE).to(DEVICE),\n",
    "    torch.zeros(LSTM_LAYERS, 1, HIDDEN_SIZE).to(DEVICE),\n",
    ")\n",
    "out, hidden = lstm(inputs2, h_c)\n",
    "print(out[-1, 0, :3])\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 16) -> None:\n",
    "        super(CNet, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnet = CNet(2 * LSTM_LAYERS * HIDDEN_SIZE, 1).to(DEVICE)\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "optimizer_lstm = optim.Adam(\n",
    "    lstm.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    # eps=1e-3\n",
    ")\n",
    "\n",
    "optimizer_cnet = optim.Adam(\n",
    "    cnet.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    # eps=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000], grad_fn=<SqueezeBackward1>)\n",
      "tensor([1.])\n",
      "1.2789769243681803e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\VSCode\\data\\tmp\\ipykernel_23300\\1046626362.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_list.append(torch.tensor(text_pipeline(_target)))\n",
      "D:\\VSCode\\data\\tmp\\ipykernel_23300\\1046626362.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  candidate_list.append(torch.tensor(text_pipeline(_candidate)))\n"
     ]
    }
   ],
   "source": [
    "optimizer_lstm.zero_grad()\n",
    "optimizer_cnet.zero_grad()\n",
    "\n",
    "target_sent, candidate_sent, score = next(iter(data_loader))\n",
    "\n",
    "\n",
    "target_hidden = lstm_loop(target_sent, lstm)\n",
    "candidate_hidden = lstm_loop(candidate_sent, lstm)\n",
    "\n",
    "inp = torch.cat([target_hidden, candidate_hidden]).to(DEVICE)\n",
    "\n",
    "out = cnet(inp.view(1, -1))\n",
    "\n",
    "print(out.squeeze(-1))\n",
    "print(score)\n",
    "\n",
    "loss = loss_fn(out.squeeze(-1), score)\n",
    "print(loss.item())\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer_lstm.step()\n",
    "optimizer_cnet.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
