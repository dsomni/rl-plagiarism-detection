{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIM = 300\n",
    "\n",
    "EMBED_DIM = GLOVE_DIM\n",
    "DROPOUT = 0.5\n",
    "\n",
    "\n",
    "NUM_HEADS = 15  # EMBED_DIM (300) should be divisible by NUM_HEADS\n",
    "LSTM_LAYERS = 1\n",
    "LSTM_H_DIM = EMBED_DIM\n",
    "\n",
    "\n",
    "OUT_CHANNELS = 3\n",
    "KERNEL_SIZE = 4\n",
    "MAX_POOL_KERNEL = 2\n",
    "MAX_POOL_STRIDE = 2\n",
    "MAX_LEN = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainable_params(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention: bool,\n",
    "        bidirectional: bool,\n",
    "        output_dim: int = 1,\n",
    "        hidden_dim: int = 128,\n",
    "    ) -> None:\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self._attention = attention\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        if self._attention:\n",
    "            self.attention = nn.MultiheadAttention(EMBED_DIM, NUM_HEADS)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            EMBED_DIM,\n",
    "            LSTM_H_DIM,\n",
    "            num_layers=LSTM_LAYERS,\n",
    "            bidirectional=self.bidirectional,\n",
    "        )\n",
    "\n",
    "        self.rnet = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(\n",
    "                LSTM_H_DIM if not self.bidirectional else LSTM_H_DIM * 2, hidden_dim\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self._attention:\n",
    "            x, _ = self.attention(x, x, x)\n",
    "        _, (x, __) = self.lstm(x)\n",
    "        x = torch.swapaxes(x, 0, 1)\n",
    "        return self.rnet(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_dim: int, output_dim: int = 1, hidden_dim: int = 128) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_dim = cnn_dim\n",
    "\n",
    "        self.conv_out_dim = (MAX_LEN - KERNEL_SIZE + 1) * OUT_CHANNELS\n",
    "        self.cnn_out_dim = (\n",
    "            int((self.conv_out_dim - MAX_POOL_KERNEL) / MAX_POOL_STRIDE) + 1\n",
    "        )\n",
    "\n",
    "        if self.cnn_dim == 1:\n",
    "            self.cnn = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=EMBED_DIM,\n",
    "                    out_channels=OUT_CHANNELS,\n",
    "                    kernel_size=KERNEL_SIZE,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(MAX_POOL_KERNEL, stride=MAX_POOL_STRIDE),\n",
    "            )\n",
    "        else:\n",
    "            self.cnn = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=EMBED_DIM,\n",
    "                    out_channels=OUT_CHANNELS,\n",
    "                    kernel_size=(2, KERNEL_SIZE),\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(start_dim=1, end_dim=2),\n",
    "                nn.MaxPool1d(MAX_POOL_KERNEL, stride=MAX_POOL_STRIDE),\n",
    "            )\n",
    "\n",
    "        self.cnet = nn.Sequential(\n",
    "            nn.Linear(self.cnn_out_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.swapaxes(x, 1, 2) if self.cnn_dim == 1 else torch.swapaxes(x, 1, 3)\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.cnet(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNetNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int = 600, output_dim: int = 1, hidden_dim: int = 128\n",
    "    ) -> None:\n",
    "        super(RNetNN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SRModelNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int = 300, hidden_size: int = 300, num_layers: int = LSTM_LAYERS\n",
    "    ) -> None:\n",
    "        super(SRModelNN, self).__init__()\n",
    "\n",
    "        self.net = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return self.net(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int = 900, output_dim: int = 2, hidden_dim: int = 16\n",
    "    ) -> None:\n",
    "        super(A2CNet, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        body_out = self.body(x)\n",
    "        return self.policy(body_out), self.value(body_out)\n",
    "\n",
    "\n",
    "class PGN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int = 900, output_dim: int = 2, hidden_dim: int = 16\n",
    "    ) -> None:\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int = 900, output_dim: int = 2, hidden_dim: int = 16\n",
    "    ) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM: 761057\n",
      "biLSTM: 1521857\n",
      "LSTM+Attention: 1122257\n",
      "biLSTM+Attention: 1883057\n"
     ]
    }
   ],
   "source": [
    "print(f\"LSTM: {get_trainable_params(LSTMNet(attention=False, bidirectional=False))}\")\n",
    "print(f\"biLSTM: {get_trainable_params(LSTMNet(attention=False, bidirectional=True))}\")\n",
    "print(\n",
    "    f\"LSTM+Attention: {get_trainable_params(LSTMNet(attention=True, bidirectional=False))}\"\n",
    ")\n",
    "print(\n",
    "    f\"biLSTM+Attention: {get_trainable_params(LSTMNet(attention=True, bidirectional=True))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1D: 60820\n",
      "CNN2D: 64420\n"
     ]
    }
   ],
   "source": [
    "print(f\"CNN1D: {get_trainable_params(CNN(cnn_dim=1))}\")\n",
    "print(f\"CNN2D: {get_trainable_params(CNN(cnn_dim=2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE: 813907, 14450\n",
      "A2C: 814196, 14739\n",
      "DQN: 813907, 14450\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"REINFORCE: {get_trainable_params(RNetNN()) + get_trainable_params(SRModelNN()) + get_trainable_params(PGN())}, {get_trainable_params(PGN())}\"\n",
    ")\n",
    "print(\n",
    "    f\"A2C: {get_trainable_params(RNetNN()) + get_trainable_params(SRModelNN()) + get_trainable_params(A2CNet())}, {get_trainable_params(A2CNet())}\"\n",
    ")\n",
    "print(\n",
    "    f\"DQN: {get_trainable_params(RNetNN()) + get_trainable_params(SRModelNN()) + get_trainable_params(DQN())}, {get_trainable_params(DQN())}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
