{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9330231d",
   "metadata": {
    "papermill": {
     "duration": 0.009174,
     "end_time": "2024-04-23T20:04:21.181898",
     "exception": false,
     "start_time": "2024-04-23T20:04:21.172724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A2C analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0272e6f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:21.202028Z",
     "iopub.status.busy": "2024-04-23T20:04:21.201032Z",
     "iopub.status.idle": "2024-04-23T20:04:25.579755Z",
     "shell.execute_reply": "2024-04-23T20:04:25.579755Z"
    },
    "papermill": {
     "duration": 4.389355,
     "end_time": "2024-04-23T20:04:25.581807",
     "exception": false,
     "start_time": "2024-04-23T20:04:21.192452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from itertools import accumulate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8a3cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:25.629918Z",
     "iopub.status.busy": "2024-04-23T20:04:25.629918Z",
     "iopub.status.idle": "2024-04-23T20:04:25.654656Z",
     "shell.execute_reply": "2024-04-23T20:04:25.653645Z"
    },
    "papermill": {
     "duration": 0.037771,
     "end_time": "2024-04-23T20:04:25.657212",
     "exception": false,
     "start_time": "2024-04-23T20:04:25.619441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b6791a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:25.676844Z",
     "iopub.status.busy": "2024-04-23T20:04:25.676844Z",
     "iopub.status.idle": "2024-04-23T20:04:25.679896Z",
     "shell.execute_reply": "2024-04-23T20:04:25.679896Z"
    },
    "papermill": {
     "duration": 0.01668,
     "end_time": "2024-04-23T20:04:25.681938",
     "exception": false,
     "start_time": "2024-04-23T20:04:25.665258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOG_PATH_PREFIX = \"../logs/a2c/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2230f",
   "metadata": {
    "papermill": {
     "duration": 0.008559,
     "end_time": "2024-04-23T20:04:25.702983",
     "exception": false,
     "start_time": "2024-04-23T20:04:25.694424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db429cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:25.728655Z",
     "iopub.status.busy": "2024-04-23T20:04:25.727648Z",
     "iopub.status.idle": "2024-04-23T20:04:25.734463Z",
     "shell.execute_reply": "2024-04-23T20:04:25.734463Z"
    },
    "papermill": {
     "duration": 0.022397,
     "end_time": "2024-04-23T20:04:25.736983",
     "exception": false,
     "start_time": "2024-04-23T20:04:25.714586",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RL_GAMMA = 1.0\n",
    "REWARD_STEPS = 0\n",
    "\n",
    "GLOVE_DIM = 300\n",
    "TRAIN_SIZE = \"md\"\n",
    "TEST_SIZE = \"sm\"\n",
    "\n",
    "EMBED_DIM = GLOVE_DIM\n",
    "LSTM_LAYERS = 1\n",
    "LSTM_H_DIM = EMBED_DIM\n",
    "RNET_DROPOUT = 0.5\n",
    "\n",
    "ENV_GAMMA = 0.1\n",
    "\n",
    "PGN_LR = 0.005\n",
    "RNET_LR = 0.001\n",
    "SRM_LR = 0.001\n",
    "\n",
    "PGN_CLIP_GRAD = 0.0\n",
    "ENTROPY_BETA = 0.001\n",
    "\n",
    "PRETRAIN_SRM_RNET_EPOCHS = 200\n",
    "PRETRAIN_PGN_EPOCHS = 100\n",
    "\n",
    "EPISODES_BATCH = 10\n",
    "PRETRAIN_SRM_RNET_BATCH = EPISODES_BATCH\n",
    "\n",
    "FEATURES = \"\"\n",
    "COMMENTS = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ba46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"23_04\"\n",
    "EPOCH = 1410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c221b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:25.848178Z",
     "iopub.status.busy": "2024-04-23T20:04:25.847166Z",
     "iopub.status.idle": "2024-04-23T20:04:25.865873Z",
     "shell.execute_reply": "2024-04-23T20:04:25.864861Z"
    },
    "papermill": {
     "duration": 0.049056,
     "end_time": "2024-04-23T20:04:25.870097",
     "exception": false,
     "start_time": "2024-04-23T20:04:25.821041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_from_experiments(experiment: str) -> None:\n",
    "    global \\\n",
    "        RL_GAMMA, \\\n",
    "        REWARD_STEPS, \\\n",
    "        GLOVE_DIM, \\\n",
    "        TRAIN_SIZE, \\\n",
    "        TEST_SIZE, \\\n",
    "        EMBED_DIM, \\\n",
    "        LSTM_LAYERS, \\\n",
    "        LSTM_H_DIM, \\\n",
    "        RNET_DROPOUT, \\\n",
    "        ENV_GAMMA, \\\n",
    "        PGN_LR, \\\n",
    "        RNET_LR, \\\n",
    "        SRM_LR, \\\n",
    "        PGN_CLIP_GRAD, \\\n",
    "        ENTROPY_BETA, \\\n",
    "        PRETRAIN_SRM_RNET_EPOCHS, \\\n",
    "        PRETRAIN_PGN_EPOCHS, \\\n",
    "        EPISODES_BATCH, \\\n",
    "        PRETRAIN_SRM_RNET_BATCH, \\\n",
    "        FEATURES, \\\n",
    "        COMMENTS\n",
    "    with open(os.path.join(\".\", LOG_PATH_PREFIX, experiment, \"configs.json\"), \"r\") as f:\n",
    "        hyper_dict = json.load(f)\n",
    "    RL_GAMMA = hyper_dict[\"RL_GAMMA\"]\n",
    "    REWARD_STEPS = hyper_dict[\"REWARD_STEPS\"]\n",
    "    GLOVE_DIM = hyper_dict[\"GLOVE_DIM\"]\n",
    "    TRAIN_SIZE = hyper_dict[\"TRAIN_SIZE\"]\n",
    "    TEST_SIZE = hyper_dict[\"TEST_SIZE\"]\n",
    "    EMBED_DIM = hyper_dict[\"EMBED_DIM\"]\n",
    "    LSTM_LAYERS = hyper_dict[\"LSTM_LAYERS\"]\n",
    "    LSTM_H_DIM = hyper_dict[\"LSTM_H_DIM\"]\n",
    "    RNET_DROPOUT = hyper_dict[\"RNET_DROPOUT\"]\n",
    "    ENV_GAMMA = hyper_dict[\"ENV_GAMMA\"]\n",
    "    PGN_LR = hyper_dict[\"PGN_LR\"]\n",
    "    RNET_LR = hyper_dict[\"RNET_LR\"]\n",
    "    SRM_LR = hyper_dict[\"SRM_LR\"]\n",
    "    PGN_CLIP_GRAD = hyper_dict[\"PGN_CLIP_GRAD\"]\n",
    "    ENTROPY_BETA = hyper_dict[\"ENTROPY_BETA\"]\n",
    "    PRETRAIN_SRM_RNET_EPOCHS = hyper_dict[\"PRETRAIN_SRM_RNET_EPOCHS\"]\n",
    "    PRETRAIN_PGN_EPOCHS = hyper_dict[\"PRETRAIN_PGN_EPOCHS\"]\n",
    "    EPISODES_BATCH = hyper_dict[\"EPISODES_BATCH\"]\n",
    "    PRETRAIN_SRM_RNET_BATCH = hyper_dict[\"PRETRAIN_SRM_RNET_BATCH\"]\n",
    "    FEATURES = hyper_dict[\"FEATURES\"]\n",
    "    COMMENTS = hyper_dict[\"COMMENTS\"]\n",
    "    print(hyper_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d907a94",
   "metadata": {
    "papermill": {
     "duration": 0.077521,
     "end_time": "2024-04-23T20:04:26.610267",
     "exception": false,
     "start_time": "2024-04-23T20:04:26.532746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81fe366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name: str, model):\n",
    "    path = os.path.join(\n",
    "        \".\", LOG_PATH_PREFIX, EXPERIMENT_NAME, \"best\", str(EPOCH), \"models\", name\n",
    "    )\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d172b92",
   "metadata": {},
   "source": [
    "## RL Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deef38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qvals(\n",
    "    rewards: list[float], gamma: float = RL_GAMMA, reward_steps: int = REWARD_STEPS\n",
    ") -> np.ndarray:\n",
    "    rw_steps = reward_steps if reward_steps != 0 else len(rewards)\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            list(\n",
    "                accumulate(\n",
    "                    reversed(rewards[i : i + rw_steps]), lambda x, y: gamma * x + y\n",
    "                )\n",
    "            )[-1]\n",
    "            for i in range(len(rewards))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8fc9e",
   "metadata": {
    "papermill": {
     "duration": 0.043267,
     "end_time": "2024-04-23T20:04:27.025295",
     "exception": false,
     "start_time": "2024-04-23T20:04:26.982028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd6e480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:27.148780Z",
     "iopub.status.busy": "2024-04-23T20:04:27.147782Z",
     "iopub.status.idle": "2024-04-23T20:04:32.465419Z",
     "shell.execute_reply": "2024-04-23T20:04:32.465419Z"
    },
    "papermill": {
     "duration": 5.358045,
     "end_time": "2024-04-23T20:04:32.470031",
     "exception": false,
     "start_time": "2024-04-23T20:04:27.111986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "global_vectors = GloVe(dim=GLOVE_DIM, cache=\"../data\")\n",
    "\n",
    "\n",
    "def text_pipeline(x):\n",
    "    return global_vectors.get_vecs_by_tokens(tokenizer(x), lower_case_backup=True)\n",
    "\n",
    "\n",
    "def tokenized_pipeline(x):\n",
    "    return global_vectors.get_vecs_by_tokens(x, lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "846f97d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:32.504546Z",
     "iopub.status.busy": "2024-04-23T20:04:32.502533Z",
     "iopub.status.idle": "2024-04-23T20:04:32.513105Z",
     "shell.execute_reply": "2024-04-23T20:04:32.512093Z"
    },
    "papermill": {
     "duration": 0.028026,
     "end_time": "2024-04-23T20:04:32.515101",
     "exception": false,
     "start_time": "2024-04-23T20:04:32.487075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_from_disk(path: str) -> np.ndarray:\n",
    "    return pd.read_csv(path).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae517db4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:32.549386Z",
     "iopub.status.busy": "2024-04-23T20:04:32.548388Z",
     "iopub.status.idle": "2024-04-23T20:04:32.561525Z",
     "shell.execute_reply": "2024-04-23T20:04:32.560569Z"
    },
    "papermill": {
     "duration": 0.034384,
     "end_time": "2024-04-23T20:04:32.563520",
     "exception": false,
     "start_time": "2024-04-23T20:04:32.529136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PlagiarismDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        targets, candidates, scores = [], [], []\n",
    "\n",
    "        for target, candidate, score in data:\n",
    "            targets.append(tokenizer(target))\n",
    "            candidates.append(tokenizer(candidate))\n",
    "            scores.append(score)\n",
    "\n",
    "        self.targets = targets\n",
    "        self.candidates = candidates\n",
    "        self.scores = np.array(scores).astype(np.float16)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scores)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, list, list]:\n",
    "        return (\n",
    "            tokenized_pipeline(self.targets[idx]).to(DEVICE),\n",
    "            tokenized_pipeline(self.candidates[idx]).to(DEVICE),\n",
    "            torch.tensor([self.scores[idx]]).float().to(DEVICE),\n",
    "            self.targets[idx],\n",
    "            self.candidates[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "931966ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:32.606938Z",
     "iopub.status.busy": "2024-04-23T20:04:32.605309Z",
     "iopub.status.idle": "2024-04-23T20:04:33.461880Z",
     "shell.execute_reply": "2024-04-23T20:04:33.458852Z"
    },
    "papermill": {
     "duration": 0.879048,
     "end_time": "2024-04-23T20:04:33.465879",
     "exception": false,
     "start_time": "2024-04-23T20:04:32.586831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_data)=147\n"
     ]
    }
   ],
   "source": [
    "test_data = PlagiarismDataset(\n",
    "    read_from_disk(f\"../generated/datasets/test_{TEST_SIZE}.csv\")\n",
    ")\n",
    "\n",
    "print(f\"{len(test_data)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8c439",
   "metadata": {
    "papermill": {
     "duration": 0.033685,
     "end_time": "2024-04-23T20:04:33.610794",
     "exception": false,
     "start_time": "2024-04-23T20:04:33.577109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RNet & SRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b161e689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:33.758273Z",
     "iopub.status.busy": "2024-04-23T20:04:33.755610Z",
     "iopub.status.idle": "2024-04-23T20:04:33.788447Z",
     "shell.execute_reply": "2024-04-23T20:04:33.784919Z"
    },
    "papermill": {
     "duration": 0.105747,
     "end_time": "2024-04-23T20:04:33.794468",
     "exception": false,
     "start_time": "2024-04-23T20:04:33.688721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNetNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 128) -> None:\n",
    "        super(RNetNN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(RNET_DROPOUT),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class RNet:\n",
    "    def __init__(self, lr: float = 1e-2, device=DEVICE):\n",
    "        self.net = RNetNN(2 * LSTM_LAYERS * LSTM_H_DIM, 1).to(device)\n",
    "        self.loss_fn = F.mse_loss\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.net.parameters(),\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "    def __call__(self, data: torch.Tensor, grad: bool = True) -> torch.Tensor:\n",
    "        if grad:\n",
    "            return self.net(data)\n",
    "        with torch.no_grad():\n",
    "            return self.net(data)\n",
    "\n",
    "\n",
    "class SRModelNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, hidden_size: int, num_layers: int = LSTM_LAYERS\n",
    "    ) -> None:\n",
    "        super(SRModelNN, self).__init__()\n",
    "\n",
    "        self.net = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=False,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return self.net(*x)\n",
    "\n",
    "\n",
    "class SRModel:\n",
    "    def __init__(self, lr: float = 1e-2, device=DEVICE):\n",
    "        self.net = SRModelNN(EMBED_DIM, LSTM_H_DIM).to(device)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.net.parameters(),\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, *data, grad: bool = True) -> torch.Tensor:\n",
    "        if grad:\n",
    "            return self.net(*data)\n",
    "        with torch.no_grad():\n",
    "            return self.net(*data)\n",
    "\n",
    "    def call_batch(self, data, grad: bool = True) -> torch.Tensor:\n",
    "        cat_data = torch.cat(data) if type(data) == list else data\n",
    "        cat_data = cat_data.view(len(data), 1, -1)\n",
    "        h_c = (\n",
    "            torch.zeros(LSTM_LAYERS, 1, LSTM_H_DIM).to(self.device),\n",
    "            torch.zeros(LSTM_LAYERS, 1, LSTM_H_DIM).to(self.device),\n",
    "        )\n",
    "        out, _ = self.__call__(cat_data, h_c, grad=grad)\n",
    "        return out[-1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ab1bb59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:33.980287Z",
     "iopub.status.busy": "2024-04-23T20:04:33.979273Z",
     "iopub.status.idle": "2024-04-23T20:04:34.001927Z",
     "shell.execute_reply": "2024-04-23T20:04:33.997943Z"
    },
    "papermill": {
     "duration": 0.140474,
     "end_time": "2024-04-23T20:04:34.064833",
     "exception": false,
     "start_time": "2024-04-23T20:04:33.924359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_sample(\n",
    "    srm: SRModel,\n",
    "    rnet: RNet,\n",
    "    target: list[torch.Tensor],\n",
    "    candidate: list[torch.Tensor],\n",
    "    train_srm: bool = True,\n",
    "    train_rnet: bool = True,\n",
    "):\n",
    "    srm_out_target = srm.call_batch(target, train_srm)\n",
    "    srm_out_candidate = srm.call_batch(candidate, train_srm)\n",
    "\n",
    "    rnet_out = rnet(\n",
    "        torch.cat([srm_out_target, srm_out_candidate]).view(1, -1), grad=train_rnet\n",
    "    )\n",
    "\n",
    "    return rnet_out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb477e9a",
   "metadata": {
    "papermill": {
     "duration": 0.049989,
     "end_time": "2024-04-23T20:04:34.321869",
     "exception": false,
     "start_time": "2024-04-23T20:04:34.271880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e27e3583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:34.413425Z",
     "iopub.status.busy": "2024-04-23T20:04:34.412458Z",
     "iopub.status.idle": "2024-04-23T20:04:34.464565Z",
     "shell.execute_reply": "2024-04-23T20:04:34.462550Z"
    },
    "papermill": {
     "duration": 0.112137,
     "end_time": "2024-04-23T20:04:34.468565",
     "exception": false,
     "start_time": "2024-04-23T20:04:34.356428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def _get_state(self) -> torch.Tensor:\n",
    "        token = self.data[self.sentence_idx][self.token_idx]\n",
    "        return torch.cat(\n",
    "            [\n",
    "                self.hs[self.sentence_idx].flatten(),\n",
    "                self.cs[self.sentence_idx].flatten(),\n",
    "                token,\n",
    "            ]\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    def _get_reward(self) -> float:\n",
    "        if not self.is_terminal():\n",
    "            return 0.0\n",
    "\n",
    "        self.statistics_dict[\"Deletions ratio\"] = (\n",
    "            self.statistics_dict[\"Deletions\"] / self.total_words\n",
    "        )\n",
    "\n",
    "        # Case when agent removes the entire sequence\n",
    "        if self.statistics_dict[\"Deletions\"] == self.total_words:\n",
    "            return 0.0\n",
    "\n",
    "        rnet_out = self.rnet(self.hs.view(1, -1), grad=False)\n",
    "\n",
    "        score_tensor = torch.FloatTensor([self.data[2]]).to(DEVICE)\n",
    "        loss = self.rnet.loss_fn(rnet_out.squeeze(-1), score_tensor)\n",
    "        self.loss = loss.item()\n",
    "\n",
    "        rnet_reward = np.log(1 - loss.item() + 1e-8)\n",
    "        deletions_reward = (\n",
    "            self.gamma * self.statistics_dict[\"Deletions\"] / self.total_words\n",
    "        )\n",
    "\n",
    "        self.statistics_dict[\"RNet reward\"] = rnet_reward\n",
    "        self.statistics_dict[\"Deletions reward\"] = deletions_reward\n",
    "\n",
    "        return rnet_reward + deletions_reward\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        srm: SRModel,\n",
    "        rnet: RNet,\n",
    "        gamma: float = ENV_GAMMA,\n",
    "        random_sampling: bool = True,\n",
    "    ) -> None:\n",
    "        self.srm = srm\n",
    "        self.rnet = rnet\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.random_sampling = random_sampling\n",
    "        self.idx = -1\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, idx: int = -1) -> torch.Tensor:\n",
    "        self.loss = -1\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.sentence_idx = 0\n",
    "        self.token_idx = 0\n",
    "\n",
    "        if self.random_sampling:\n",
    "            self.idx = np.random.randint(len(self.dataset))\n",
    "        elif idx >= 0:\n",
    "            self.idx = idx\n",
    "        else:\n",
    "            self.idx = (self.idx + 1) % len(self.dataset)\n",
    "\n",
    "        self.data = self.dataset[self.idx]\n",
    "\n",
    "        self.total_words = len(self.data[0]) + len(self.data[1])\n",
    "\n",
    "        self.hs = torch.zeros((2, LSTM_LAYERS, LSTM_H_DIM)).to(DEVICE)\n",
    "        self.cs = torch.zeros((2, LSTM_LAYERS, LSTM_H_DIM)).to(DEVICE)\n",
    "\n",
    "        self.used_tokens: tuple[list[torch.Tensor], list[torch.Tensor], float] = (\n",
    "            [],\n",
    "            [],\n",
    "            self.data[2].item(),\n",
    "        )\n",
    "\n",
    "        self.used_pure_tokens: tuple[list[str], list[str]] = ([], [])\n",
    "        self.deleted_tokens_dict = {}\n",
    "\n",
    "        self.statistics_dict = {\n",
    "            \"Initial Target length\": len(self.data[0]),\n",
    "            \"Initial Candidate length\": len(self.data[1]),\n",
    "            \"Processed Target length\": 0,\n",
    "            \"Processed Candidate length\": 0,\n",
    "            \"Deletions\": 0,\n",
    "            \"RNet reward\": 0.0,\n",
    "            \"Deletions reward\": 0.0,\n",
    "            \"Deletions ratio\": 0.0,\n",
    "        }\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self) -> torch.Tensor:\n",
    "        return self._get_state()\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self.sentence_idx == 1 and self.token_idx == (len(self.data[1]) - 1)\n",
    "\n",
    "    def interact(self, action: int) -> tuple[torch.Tensor, float, bool]:\n",
    "        # 0 - retain\n",
    "        # 1 - delete\n",
    "\n",
    "        if self.is_terminal():\n",
    "            return self._get_state(), 0, self.is_terminal()\n",
    "\n",
    "        if action == 1:\n",
    "            self.statistics_dict[\"Deletions\"] += 1\n",
    "            pure_token = self.data[self.sentence_idx + 3][self.token_idx].lower()\n",
    "            self.deleted_tokens_dict[pure_token] = (\n",
    "                self.deleted_tokens_dict.get(pure_token, 0) + 1\n",
    "            )\n",
    "            self.used_pure_tokens[self.sentence_idx].append(\"DELETED\")\n",
    "\n",
    "        elif action == 0:\n",
    "            pure_token = self.data[self.sentence_idx + 3][self.token_idx].lower()\n",
    "            self.used_pure_tokens[self.sentence_idx].append(pure_token)\n",
    "\n",
    "            if self.sentence_idx == 0:\n",
    "                self.statistics_dict[\"Processed Target length\"] += 1\n",
    "            else:\n",
    "                self.statistics_dict[\"Processed Candidate length\"] += 1\n",
    "\n",
    "            token = self.data[self.sentence_idx][self.token_idx]\n",
    "            self.used_tokens[self.sentence_idx].append(  # type: ignore\n",
    "                token.clone().detach()\n",
    "            )\n",
    "\n",
    "            h_c = (\n",
    "                self.hs[self.sentence_idx].clone().detach(),\n",
    "                self.cs[self.sentence_idx].clone().detach(),\n",
    "            )\n",
    "\n",
    "            _, (h, c) = self.srm(token.view(1, -1), h_c, grad=False)\n",
    "            self.hs[self.sentence_idx] = h.clone().detach()\n",
    "            self.cs[self.sentence_idx] = c.clone().detach()\n",
    "\n",
    "        self.steps += 1\n",
    "        self.token_idx += 1\n",
    "        if self.sentence_idx == 0 and self.token_idx >= len(self.data[0]):\n",
    "            self.sentence_idx = 1\n",
    "            self.token_idx = 0\n",
    "\n",
    "        return self._get_state(), self._get_reward(), self.is_terminal()\n",
    "\n",
    "    def get_used_tokens(self) -> tuple[list[torch.Tensor], list[torch.Tensor], float]:\n",
    "        if len(self.used_tokens[0]) == 0:\n",
    "            self.used_tokens[0].append(torch.zeros(LSTM_LAYERS, LSTM_H_DIM).to(DEVICE))\n",
    "        if len(self.used_tokens[1]) == 0:\n",
    "            self.used_tokens[1].append(torch.zeros(LSTM_LAYERS, LSTM_H_DIM).to(DEVICE))\n",
    "        return self.used_tokens\n",
    "\n",
    "    def get_observation_shape(self) -> int:\n",
    "        return 2 * LSTM_LAYERS * LSTM_H_DIM + EMBED_DIM\n",
    "\n",
    "    def get_actions_shape(self) -> int:\n",
    "        return 2\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_action() -> int:\n",
    "        return np.random.choice([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4e353",
   "metadata": {
    "papermill": {
     "duration": 0.034548,
     "end_time": "2024-04-23T20:04:34.550112",
     "exception": false,
     "start_time": "2024-04-23T20:04:34.515564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PGN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7da68b81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:34.598118Z",
     "iopub.status.busy": "2024-04-23T20:04:34.597117Z",
     "iopub.status.idle": "2024-04-23T20:04:34.609194Z",
     "shell.execute_reply": "2024-04-23T20:04:34.607182Z"
    },
    "papermill": {
     "duration": 0.038085,
     "end_time": "2024-04-23T20:04:34.612197",
     "exception": false,
     "start_time": "2024-04-23T20:04:34.574112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class A2CNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 16) -> None:\n",
    "        super(A2CNet, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        body_out = self.body(x)\n",
    "        return self.policy(body_out), self.value(body_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbd774",
   "metadata": {
    "papermill": {
     "duration": 0.02301,
     "end_time": "2024-04-23T20:04:34.664734",
     "exception": false,
     "start_time": "2024-04-23T20:04:34.641724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "848aa48e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:34.846047Z",
     "iopub.status.busy": "2024-04-23T20:04:34.845036Z",
     "iopub.status.idle": "2024-04-23T20:04:34.859386Z",
     "shell.execute_reply": "2024-04-23T20:04:34.858374Z"
    },
    "papermill": {
     "duration": 0.168172,
     "end_time": "2024-04-23T20:04:34.863910",
     "exception": false,
     "start_time": "2024-04-23T20:04:34.695738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def choose_action(self, action_logits):\n",
    "        return random.choices(range(len(action_logits)), F.softmax(action_logits, dim=0))[\n",
    "            0\n",
    "        ]\n",
    "\n",
    "    def choose_optimal_action(self, action_logits) -> int:\n",
    "        return int(np.argmax(F.softmax(action_logits, dim=0).cpu()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35aaec",
   "metadata": {
    "papermill": {
     "duration": 0.01754,
     "end_time": "2024-04-23T20:04:35.092555",
     "exception": false,
     "start_time": "2024-04-23T20:04:35.075015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a9191ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:35.141638Z",
     "iopub.status.busy": "2024-04-23T20:04:35.140640Z",
     "iopub.status.idle": "2024-04-23T20:04:35.163487Z",
     "shell.execute_reply": "2024-04-23T20:04:35.160945Z"
    },
    "papermill": {
     "duration": 0.054516,
     "end_time": "2024-04-23T20:04:35.169509",
     "exception": false,
     "start_time": "2024-04-23T20:04:35.114993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    srm: SRModel,\n",
    "    rnet: RNet,\n",
    "    pgn: nn.Module,\n",
    "    agent: Agent,\n",
    "    eval_env: Env,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    srm.net.eval()\n",
    "    rnet.net.eval()\n",
    "    pgn.eval()\n",
    "\n",
    "    total_len = len(eval_env.dataset)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    iteration = 0\n",
    "    state = eval_env.reset(0)\n",
    "\n",
    "    targets = []\n",
    "    candidates = []\n",
    "\n",
    "    with tqdm(total=total_len, desc=\"Evaluation\") as loop:\n",
    "        while iteration < total_len:\n",
    "            with torch.no_grad():\n",
    "                action_logits = pgn(state)[0]\n",
    "            action = agent.choose_optimal_action(action_logits)\n",
    "            state2, _, done = eval_env.interact(action)\n",
    "            state = state2.clone().detach()\n",
    "\n",
    "            if done:\n",
    "                iteration += 1\n",
    "                losses.append(eval_env.loss)\n",
    "\n",
    "                t, c = eval_env.used_pure_tokens\n",
    "                targets.append(t)\n",
    "                candidates.append(c)\n",
    "\n",
    "                state = eval_env.reset()\n",
    "\n",
    "                loop.update(1)\n",
    "\n",
    "    return targets, candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37e605",
   "metadata": {},
   "source": [
    "## Load & prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f3a1d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean init target len = 38.65\n",
      "Mean init candidate len = 40.93\n"
     ]
    }
   ],
   "source": [
    "initial_targets, initial_candidates = zip(*[(x[3], x[4]) for x in test_data])\n",
    "\n",
    "itl = np.mean([len(x) for x in initial_targets])\n",
    "icl = np.mean([len(x) for x in initial_candidates])\n",
    "\n",
    "print(f\"Mean init target len = {itl:.2f}\")\n",
    "print(f\"Mean init candidate len = {icl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf96c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data)=17\n"
     ]
    }
   ],
   "source": [
    "ANALYSIS_PATH = \"../generated/structure_analysis\"\n",
    "\n",
    "\n",
    "def define_analytical_data(name: str, sentences: list[str]):\n",
    "    d = {i: {\"initial\": s} for i, s in enumerate(sentences)}\n",
    "    with open(os.path.join(ANALYSIS_PATH, f\"{name}.json\"), \"w\") as f:\n",
    "        json.dump(d, f, indent=2)\n",
    "\n",
    "\n",
    "def convert_csv(path: str, restrictions: tuple[int, int] = (0, 100000)) -> list[str]:\n",
    "    df = pd.read_csv(path)\n",
    "    data = []\n",
    "    for _, r in df.iterrows():\n",
    "        for t in r:\n",
    "            if len(t) > restrictions[1] or len(t) < restrictions[0]:\n",
    "                continue\n",
    "            data.append(str(t))\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_csvs(restrictions: tuple[int, int]) -> list[str]:\n",
    "    return [\n",
    "        *convert_csv(\"../generated/test1/6.csv\", restrictions=restrictions),\n",
    "        *convert_csv(\"../generated/test2/6.csv\", restrictions=restrictions),\n",
    "        *convert_csv(\"../generated/train1/119.csv\", restrictions=restrictions),\n",
    "        *convert_csv(\"../generated/train2/119.csv\", restrictions=restrictions),\n",
    "    ]\n",
    "\n",
    "\n",
    "restrictions = (120, 122)\n",
    "data = convert_csvs(restrictions=restrictions)\n",
    "print(f\"{len(data)=}\")\n",
    "define_analytical_data(\n",
    "    f\"{restrictions[0]}_{restrictions[1]}\", convert_csvs(restrictions=restrictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5d602",
   "metadata": {
    "papermill": {
     "duration": 0.042756,
     "end_time": "2024-04-23T20:04:35.258018",
     "exception": false,
     "start_time": "2024-04-23T20:04:35.215262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406d5fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T20:04:35.364807Z",
     "iopub.status.busy": "2024-04-23T20:04:35.363817Z",
     "iopub.status.idle": "2024-04-23T20:04:35.397070Z",
     "shell.execute_reply": "2024-04-23T20:04:35.396057Z"
    },
    "papermill": {
     "duration": 0.0747,
     "end_time": "2024-04-23T20:04:35.404678",
     "exception": false,
     "start_time": "2024-04-23T20:04:35.329978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RL_GAMMA': 1.0, 'REWARD_STEPS': 0, 'GLOVE_DIM': 300, 'TRAIN_SIZE': 'md', 'TEST_SIZE': 'sm', 'EMBED_DIM': 300, 'LSTM_LAYERS': 1, 'LSTM_H_DIM': 300, 'RNET_DROPOUT': 0.5, 'ENV_GAMMA': 0.1, 'PGN_LR': 0.005, 'RNET_LR': 0.001, 'SRM_LR': 0.001, 'PGN_CLIP_GRAD': 0.0, 'ENTROPY_BETA': 0.001, 'PRETRAIN_SRM_RNET_EPOCHS': 200, 'PRETRAIN_PGN_EPOCHS': 100, 'EPISODES_BATCH': 10, 'PRETRAIN_SRM_RNET_BATCH': 10, 'FEATURES': '', 'COMMENTS': ''}\n"
     ]
    }
   ],
   "source": [
    "load_from_experiments(EXPERIMENT_NAME)\n",
    "\n",
    "rnet = RNet(lr=RNET_LR)\n",
    "srm = SRModel(lr=SRM_LR)\n",
    "\n",
    "env = Env(test_data, srm, rnet, random_sampling=False)\n",
    "agent = Agent()\n",
    "\n",
    "pgn = A2CNet(\n",
    "    input_dim=env.get_observation_shape(), output_dim=env.get_actions_shape()\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "load_model(\"pgn\", pgn)\n",
    "load_model(\"rnet\", rnet.net)\n",
    "load_model(\"srm\", srm.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c3768c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 147/147 [00:12<00:00, 11.77it/s]\n"
     ]
    }
   ],
   "source": [
    "targets, candidates = evaluate(srm, rnet, pgn, agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61d910c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean target len = 12.54 | 26.10 ~ 67.54% deleted\n",
      "Mean candidate len = 12.63 | 28.30 ~ 69.14% deleted\n"
     ]
    }
   ],
   "source": [
    "tl = np.mean([len([w for w in x if w != \"DELETED\"]) for x in targets])\n",
    "cl = np.mean([len([w for w in x if w != \"DELETED\"]) for x in candidates])\n",
    "\n",
    "print(f\"Mean target len = {tl:.2f} | {itl-tl:.2f} ~ {100-tl/itl*100:.2f}% deleted\")\n",
    "print(f\"Mean candidate len = {cl:.2f} | {icl-cl:.2f} ~ {100-cl/icl*100:.2f}% deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb31ff5",
   "metadata": {},
   "source": [
    "## Custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06d9dad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 17/17 [00:00<00:00, 31.25it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_and_save(name: str):\n",
    "    with open(os.path.join(ANALYSIS_PATH, f\"{name}.json\"), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    sentences = [x[\"initial\"] for x in data.values()]\n",
    "\n",
    "    custom_data = PlagiarismDataset(\n",
    "        [(s, \"a\", 0) for s in sentences],\n",
    "    )\n",
    "\n",
    "    custom_env = Env(custom_data, srm, rnet, random_sampling=False)\n",
    "    c_targets, _ = evaluate(srm, rnet, pgn, agent, custom_env)\n",
    "\n",
    "    for idx, t in enumerate(c_targets):\n",
    "        data[str(list(data.keys())[idx])][\"a2c\"] = \" \".join(t)\n",
    "\n",
    "    with open(os.path.join(ANALYSIS_PATH, f\"{name}.json\"), \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "generate_and_save(\"120_122\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2401.302006,
   "end_time": "2024-04-23T20:44:18.625533",
   "environment_variables": {},
   "exception": null,
   "input_path": "./3.2-dqn.ipynb",
   "output_path": "./3.2-dqn.ipynb",
   "parameters": {
    "EPOCHS": 1500,
    "EVAL_PERIOD": 10,
    "EXPERIMENT_NAME": "23_04_2",
    "LOG_PERIOD": 10
   },
   "start_time": "2024-04-23T20:04:17.323527",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
